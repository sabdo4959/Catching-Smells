"""
LLM API í˜¸ì¶œ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ

OpenAI APIì™€ Ollama APIë¥¼ ì§€ì›í•˜ì—¬ ë‹¤ì–‘í•œ LLM ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import logging
import os
import re
from typing import Optional, Dict, Any, List
import json
import time
from enum import Enum

try:
    from openai import OpenAI
    openai_available = True
except ImportError:
    OpenAI = None
    openai_available = False

try:
    import requests
    requests_available = True
except ImportError:
    requests_available = False


class LLMProvider(Enum):
    """ì§€ì›ë˜ëŠ” LLM ì œê³µì"""
    OPENAI = "openai"
    OLLAMA = "ollama"


# Ollama ì§€ì› ëª¨ë¸ ëª©ë¡
OLLAMA_MODELS = {
    "llama3.1:8b": "llama3.1:8b-instruct-fp16",
    "codegemma:7b": "codegemma:7b-instruct-v1.1-fp16",
    "codellama:7b": "codellama:7b-instruct-fp16"
}

# OpenAI ì§€ì› ëª¨ë¸ ëª©ë¡
OPENAI_MODELS = {
    "gpt-4o-mini": "gpt-4o-mini",
    "gpt-4o": "gpt-4o", 
    "gpt-4-turbo": "gpt-4-turbo-preview",
    "gpt-4": "gpt-4",
    "gpt-3.5-turbo": "gpt-3.5-turbo"
}


class LLMAPIError(Exception):
    """LLM API ê´€ë ¨ ì˜ˆì™¸"""
    pass


def call_openai_api(
    prompt: str,
    model: str = "gpt-4o-mini",
    max_tokens: int = 2000,
    temperature: float = 0.0,
    api_key: Optional[str] = None
) -> Optional[str]:
    """
    OpenAI APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤.
    
    Args:
        prompt: í”„ë¡¬í”„íŠ¸
        model: ì‚¬ìš©í•  ëª¨ë¸ëª…
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
        temperature: ì‘ë‹µì˜ ëœë¤ì„± (0.0 ~ 1.0)
        api_key: API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
        
    Returns:
        Optional[str]: LLM ì‘ë‹µ (ì‹¤íŒ¨ ì‹œ None)
    """
    logger = logging.getLogger(__name__)
    
    if not openai_available:
        logger.error("OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        return None
    
    try:
        # API í‚¤ ìš°ì„ ìˆœìœ„: íŒŒë¼ë¯¸í„° > í™˜ê²½ë³€ìˆ˜ > ê¸°ë³¸ê°’
        final_api_key = api_key or os.getenv("OPENAI_API_KEY") or ""
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = OpenAI(api_key=final_api_key)
        
        logger.info(f"OpenAI API í˜¸ì¶œ ì‹œì‘ (ëª¨ë¸: {model})")
        
        # API í˜¸ì¶œ
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=max_tokens,
            temperature=temperature,
            timeout=180  # 60ì´ˆì—ì„œ 180ì´ˆë¡œ ì¦ê°€ (ë³µì¡í•œ íŒŒì¼ ì²˜ë¦¬ ìœ„í•´)
        )
        
        # ì‘ë‹µ ì¶”ì¶œ
        if response and response.choices:
            content = response.choices[0].message.content
            logger.info("OpenAI API í˜¸ì¶œ ì„±ê³µ")
            return content
        else:
            logger.error("OpenAI API ì‘ë‹µì´ ë¹„ì–´ìˆìŒ")
            return None
            
    except Exception as e:
        logger.error(f"OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None


def call_ollama_api(
    prompt: str,
    model: str = "llama3.1:8b-instruct-fp16",
    ollama_url: str = "http://115.145.178.160:11434/api/chat",
    temperature: float = 0.1,
    timeout: int = 300
) -> Optional[str]:
    """
    Ollama APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤.
    
    Args:
        prompt: í”„ë¡¬í”„íŠ¸
        model: ì‚¬ìš©í•  ëª¨ë¸ëª…
        ollama_url: Ollama ì„œë²„ URL
        temperature: ì‘ë‹µì˜ ëœë¤ì„± (0.0 ~ 1.0)
        timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        
    Returns:
        Optional[str]: LLM ì‘ë‹µ (ì‹¤íŒ¨ ì‹œ None)
    """
    logger = logging.getLogger(__name__)
    
    if not requests_available:
        logger.error("requests ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        return None
    
    payload = {
        "model": model,
        "messages": [
            {
                "role": "user", 
                "content": prompt
            }
        ],
        "stream": False,
        "options": {
            "temperature": temperature
        }
    }
    
    try:
        logger.info(f"Ollama API í˜¸ì¶œ ì‹œì‘ (ëª¨ë¸: {model}, URL: {ollama_url})")
        
        response = requests.post(ollama_url, json=payload, timeout=timeout)
        response.raise_for_status()
        
        result = response.json()
        content = result.get('message', {}).get('content', '').strip()
        
        # YAML ì½”ë“œ ë¸”ë¡ì´ ìˆë‹¤ë©´ ì œê±°
        if content.startswith('```yaml'):
            content = content[7:]
        if content.startswith('```'):
            content = content[3:]
        if content.endswith('```'):
            content = content[:-3]
            
        content = content.strip()
        
        logger.info("Ollama API í˜¸ì¶œ ì„±ê³µ")
        return content
        
    except requests.exceptions.RequestException as e:
        logger.error(f"Ollama API í˜¸ì¶œ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
        return None
    except Exception as e:
        logger.error(f"Ollama API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None


def call_llm(
    prompt: str,
    provider: LLMProvider = LLMProvider.OPENAI,
    model: Optional[str] = None,
    max_tokens: int = 2000,
    temperature: float = 0.1,
    api_key: Optional[str] = None,
    ollama_url: str = "http://115.145.178.160:11434/api/chat",
    timeout: int = 300
) -> Optional[str]:
    """
    LLM APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤. (í†µí•© ì¸í„°í˜ì´ìŠ¤)
    
    Args:
        prompt: í”„ë¡¬í”„íŠ¸
        provider: LLM ì œê³µì (OPENAI ë˜ëŠ” OLLAMA)
        model: ì‚¬ìš©í•  ëª¨ë¸ëª… (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        max_tokens: ìµœëŒ€ í† í° ìˆ˜ (OpenAIë§Œ í•´ë‹¹)
        temperature: ì‘ë‹µì˜ ëœë¤ì„± (0.0 ~ 1.0)
        api_key: API í‚¤ (OpenAIë§Œ í•´ë‹¹, Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
        ollama_url: Ollama ì„œë²„ URL (Ollamaë§Œ í•´ë‹¹)
        timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        
    Returns:
        Optional[str]: LLM ì‘ë‹µ (ì‹¤íŒ¨ ì‹œ None)
    """
    logger = logging.getLogger(__name__)
    
    # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
    if model is None:
        if provider == LLMProvider.OPENAI:
            model = "gpt-4o-mini"
        elif provider == LLMProvider.OLLAMA:
            model = "llama3.1:8b-instruct-fp16"
    
    # ì œê³µìë³„ API í˜¸ì¶œ
    if provider == LLMProvider.OPENAI:
        logger.info(f"OpenAI ì œê³µìë¡œ LLM í˜¸ì¶œ: {model}")
        return call_openai_api(
            prompt=prompt,
            model=model,
            max_tokens=max_tokens,
            temperature=temperature,
            api_key=api_key
        )
    elif provider == LLMProvider.OLLAMA:
        logger.info(f"Ollama ì œê³µìë¡œ LLM í˜¸ì¶œ: {model}")
        return call_ollama_api(
            prompt=prompt,
            model=model,
            ollama_url=ollama_url,
            temperature=temperature,
            timeout=timeout
        )
    else:
        logger.error(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” LLM ì œê³µì: {provider}")
        return None


def call_llm_with_retry(
    prompt: str,
    max_retries: int = 3,
    retry_delay: float = 1.0,
    **kwargs
) -> Optional[str]:
    """
    ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ LLM API í˜¸ì¶œ.
    
    Args:
        prompt: í”„ë¡¬í”„íŠ¸
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        retry_delay: ì¬ì‹œë„ ê°„ê²© (ì´ˆ)
        **kwargs: call_llmì— ì „ë‹¬ë  ì¶”ê°€ ì¸ìë“¤
        
    Returns:
        Optional[str]: LLM ì‘ë‹µ (ì‹¤íŒ¨ ì‹œ None)
    """
    logger = logging.getLogger(__name__)
    
    for attempt in range(max_retries + 1):
        try:
            result = call_llm(prompt, **kwargs)
            if result:
                return result
            
            if attempt < max_retries:
                logger.warning(f"LLM API í˜¸ì¶œ ì‹¤íŒ¨, {retry_delay}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})")
                time.sleep(retry_delay)
                retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
            
        except Exception as e:
            logger.error(f"LLM API í˜¸ì¶œ ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
            if attempt < max_retries:
                time.sleep(retry_delay)
                retry_delay *= 2
    
    logger.error(f"ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨, ì´ {max_retries + 1}íšŒ ì‹œë„í•¨")
    return None


def call_llm_batch(
    prompts: List[str],
    batch_size: int = 5,
    delay_between_batches: float = 1.0,
    **kwargs
) -> List[Optional[str]]:
    """
    ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        prompts: í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
        batch_size: ë°°ì¹˜ í¬ê¸°
        delay_between_batches: ë°°ì¹˜ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ)
        **kwargs: call_llmì— ì „ë‹¬ë  ì¶”ê°€ ì¸ìë“¤
        
    Returns:
        List[Optional[str]]: ì‘ë‹µ ë¦¬ìŠ¤íŠ¸
    """
    logger = logging.getLogger(__name__)
    
    results = []
    total_prompts = len(prompts)
    
    logger.info(f"ë°°ì¹˜ LLM í˜¸ì¶œ ì‹œì‘: {total_prompts}ê°œ í”„ë¡¬í”„íŠ¸, ë°°ì¹˜ í¬ê¸°: {batch_size}")
    
    for i in range(0, total_prompts, batch_size):
        batch = prompts[i:i + batch_size]
        batch_results = []
        
        logger.info(f"ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘: {len(batch)}ê°œ í”„ë¡¬í”„íŠ¸")
        
        for j, prompt in enumerate(batch):
            logger.debug(f"  í”„ë¡¬í”„íŠ¸ {i + j + 1}/{total_prompts} ì²˜ë¦¬ ì¤‘...")
            result = call_llm(prompt, **kwargs)
            batch_results.append(result)
            
            # ë°°ì¹˜ ë‚´ ìš”ì²­ ê°„ ì§€ì—° (API ë ˆì´íŠ¸ ì œí•œ ë°©ì§€)
            if j < len(batch) - 1:
                time.sleep(0.1)
        
        results.extend(batch_results)
        
        # ë°°ì¹˜ ê°„ ì§€ì—°
        if i + batch_size < total_prompts:
            logger.info(f"ë‹¤ìŒ ë°°ì¹˜ ì „ {delay_between_batches}ì´ˆ ëŒ€ê¸°...")
            time.sleep(delay_between_batches)
    
    logger.info(f"ë°°ì¹˜ LLM í˜¸ì¶œ ì™„ë£Œ: {len(results)}ê°œ ì‘ë‹µ")
    return results


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ í•¨ìˆ˜ë“¤
def call_openai(prompt: str, **kwargs) -> Optional[str]:
    """OpenAI API í˜¸ì¶œ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    return call_llm(prompt, provider=LLMProvider.OPENAI, **kwargs)


def call_ollama(prompt: str, **kwargs) -> Optional[str]:
    """Ollama API í˜¸ì¶œ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    return call_llm(prompt, provider=LLMProvider.OLLAMA, **kwargs)


# ê¸°ì¡´ í•¨ìˆ˜ëª…ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ (ê¸°ì¡´ ì½”ë“œê°€ ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ)
def call_llm_openai(prompt: str, **kwargs) -> Optional[str]:
    """ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±ì„ ìœ„í•œ OpenAI í˜¸ì¶œ"""
    return call_openai_api(prompt, **kwargs)


def get_available_providers() -> List[str]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µì ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    providers = []
    
    if openai_available:
        providers.append("openai")
    
    if requests_available:
        providers.append("ollama")
    
    return providers


def create_workflow_repair_prompt(workflow_content: str) -> str:
    """ì›Œí¬í”Œë¡œìš° ìˆ˜ë¦¬ë¥¼ ìœ„í•œ í‘œì¤€ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return f"""
You are an expert in GitHub Actions workflow optimization and security. Please analyze and improve the following GitHub Actions workflow file to fix common issues and smells.

Focus on improving:
1. Security issues (outdated actions, permissions)
2. Performance issues (timeout settings, caching)
3. Reliability issues (race conditions, resource limits)
4. Best practices (concurrency, error handling)

Original workflow:
```yaml
{workflow_content}
```

Please provide ONLY the improved YAML content without any explanations or markdown formatting. The output should be valid YAML that can be directly saved to a file.
"""


def extract_code_from_response(response: str, language: str = "yaml") -> Optional[str]:
    """
    LLM ì‘ë‹µì—ì„œ ì½”ë“œ ë¸”ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        response: LLM ì‘ë‹µ í…ìŠ¤íŠ¸
        language: ì¶”ì¶œí•  ì½”ë“œ ì–¸ì–´ (yaml, python ë“±)
        
    Returns:
        Optional[str]: ì¶”ì¶œëœ ì½”ë“œ (ì‹¤íŒ¨ ì‹œ None)
    """
    if not response:
        return None
    
    # ì–¸ì–´ë³„ ì½”ë“œ ë¸”ë¡ íŒ¨í„´
    patterns = [
        f"```{language}\\s*\\n(.*?)\\n```",  # ```yaml ... ```
        f"```\\s*\\n(.*?)\\n```",           # ``` ... ```
        f"```{language}(.*?)```",           # ```yaml...```
        f"```(.*?)```"                     # ```...```
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, response, re.DOTALL | re.IGNORECASE)
        if matches:
            # ê°€ì¥ ê¸´ ë§¤ì¹˜ë¥¼ ì„ íƒ (ë” ì™„ì „í•  ê°€ëŠ¥ì„±)
            code = max(matches, key=len).strip()
            return code
    
    # ì½”ë“œ ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì „ì²´ ì‘ë‹µ ë°˜í™˜ (ì´ë¯¸ ì½”ë“œì¼ ìˆ˜ ìˆìŒ)
    return response.strip()


def validate_llm_response(response: str) -> bool:
    """
    LLM ì‘ë‹µì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Args:
        response: ê²€ì¦í•  ì‘ë‹µ
        
    Returns:
        bool: ìœ íš¨í•˜ë©´ True
    """
    if not response or not response.strip():
        return False
    
    # ê¸°ë³¸ì ì¸ ìœ íš¨ì„± ê²€ì‚¬
    if len(response.strip()) < 10:
        return False
    
    # ì—ëŸ¬ ë©”ì‹œì§€ íŒ¨í„´ ê²€ì‚¬
    error_patterns = [
        r"I cannot|I can't|I'm sorry|I apologize",
        r"error|Error|ERROR",
        r"invalid|Invalid|INVALID"
    ]
    
    for pattern in error_patterns:
        if re.search(pattern, response, re.IGNORECASE):
            return False
    
    return True


def format_prompt_for_repair(workflow_content: str, issues: List[str] = None) -> str:
    """
    ì›Œí¬í”Œë¡œìš° ìˆ˜ë¦¬ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
    
    Args:
        workflow_content: ì›Œí¬í”Œë¡œìš° ë‚´ìš©
        issues: ë°œê²¬ëœ ì´ìŠˆ ëª©ë¡
        
    Returns:
        str: í¬ë§·íŒ…ëœ í”„ë¡¬í”„íŠ¸
    """
    base_prompt = f"""
You are an expert in GitHub Actions workflow optimization and security. Please analyze and improve the following GitHub Actions workflow file to fix common issues and smells.

Focus on improving:
1. Security issues (outdated actions, permissions)
2. Performance issues (timeout settings, caching)
3. Reliability issues (race conditions, resource limits)
4. Best practices (concurrency, error handling)
"""
    
    if issues:
        issues_text = "\n".join(f"- {issue}" for issue in issues)
        base_prompt += f"\n\nSpecific issues to address:\n{issues_text}"
    
    base_prompt += f"""

Original workflow:
```yaml
{workflow_content}
```

Please provide ONLY the improved YAML content without any explanations or markdown formatting. The output should be valid YAML that can be directly saved to a file.
"""
    
    return base_prompt


def get_model_info() -> Dict[str, Any]:
    """
    í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        Dict[str, Any]: ëª¨ë¸ ì •ë³´
    """
    provider = _get_current_provider()
    
    if provider == LLMProvider.OPENAI:
        model_key = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        actual_model = OPENAI_MODELS.get(model_key, model_key)
        return {
            "provider": "openai",
            "model_key": model_key,
            "actual_model": actual_model,
            "available": openai_available,
            "supported_models": list(OPENAI_MODELS.keys())
        }
    elif provider == LLMProvider.OLLAMA:
        model_key = os.getenv("OLLAMA_MODEL", "llama3.1:8b")
        actual_model = OLLAMA_MODELS.get(model_key, model_key)
        return {
            "provider": "ollama",
            "model_key": model_key,
            "actual_model": actual_model,
            "url": os.getenv("OLLAMA_URL", "http://115.145.178.160:11434/api/chat"),
            "available": requests_available,
            "supported_models": list(OLLAMA_MODELS.keys())
        }
    else:
        return {"provider": "unknown", "available": False}


def get_available_ollama_models() -> List[str]:
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        List[str]: ëª¨ë¸ í‚¤ ëª©ë¡
    """
    return list(OLLAMA_MODELS.keys())


def get_available_openai_models() -> List[str]:
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ OpenAI ëª¨ë¸ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        List[str]: ëª¨ë¸ í‚¤ ëª©ë¡
    """
    return list(OPENAI_MODELS.keys())


def validate_model_for_provider(provider: LLMProvider, model: str) -> bool:
    """
    íŠ¹ì • ì œê³µìì— ëŒ€í•´ ëª¨ë¸ì´ ìœ íš¨í•œì§€ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Args:
        provider: LLM ì œê³µì
        model: ëª¨ë¸ í‚¤
        
    Returns:
        bool: ìœ íš¨í•˜ë©´ True
    """
    if provider == LLMProvider.OPENAI:
        return model in OPENAI_MODELS
    elif provider == LLMProvider.OLLAMA:
        return model in OLLAMA_MODELS
    return False


def estimate_token_cost(prompt: str, max_tokens: int = 2000) -> Dict[str, float]:
    """
    í† í° ë¹„ìš©ì„ ì¶”ì •í•©ë‹ˆë‹¤ (OpenAI ê¸°ì¤€).
    
    Args:
        prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
        max_tokens: ìµœëŒ€ ì¶œë ¥ í† í°
        
    Returns:
        Dict[str, float]: ì˜ˆìƒ ë¹„ìš© ì •ë³´
    """
    # ëŒ€ëµì ì¸ í† í° ê³„ì‚° (1 í† í° â‰ˆ 4ê¸€ì)
    input_tokens = len(prompt) // 4
    output_tokens = max_tokens
    
    # GPT-4o-mini ê°€ê²© (2024ë…„ ê¸°ì¤€, $0.00015/1K input, $0.0006/1K output)
    input_cost = (input_tokens / 1000) * 0.00015
    output_cost = (output_tokens / 1000) * 0.0006
    total_cost = input_cost + output_cost
    
    return {
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "input_cost_usd": input_cost,
        "output_cost_usd": output_cost,
        "total_cost_usd": total_cost
    }


def _get_current_provider() -> LLMProvider:
    """í™˜ê²½ë³€ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í˜„ì¬ ì‚¬ìš©í•  LLM ì œê³µìë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    provider_env = os.getenv("LLM_PROVIDER", "openai").lower()
    
    if provider_env == "ollama":
        return LLMProvider.OLLAMA
    else:
        return LLMProvider.OPENAI


def call_llm(
    prompt: str,
    model: str = None,
    max_tokens: int = 2000,
    temperature: float = 0.1,
    api_key: Optional[str] = None
) -> Optional[str]:
    """
    ê¸°ì¡´ main.pyì™€ í˜¸í™˜ë˜ëŠ” LLM í˜¸ì¶œ í•¨ìˆ˜.
    í™˜ê²½ë³€ìˆ˜ LLM_PROVIDERë¡œ ì œê³µì ì„ íƒ ê°€ëŠ¥.
    
    í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì˜ˆì‹œ:
    - export LLM_PROVIDER=ollama
    - export OLLAMA_MODEL=llama3.1:8b
    - export OLLAMA_URL=http://115.145.178.160:11434/api/chat
    
    Args:
        prompt: í”„ë¡¬í”„íŠ¸
        model: ì‚¬ìš©í•  ëª¨ë¸ëª… (í™˜ê²½ë³€ìˆ˜ë¡œ ì¬ì •ì˜ ê°€ëŠ¥)
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
        temperature: ì‘ë‹µì˜ ëœë¤ì„±
        api_key: API í‚¤
        
    Returns:
        Optional[str]: LLM ì‘ë‹µ
    """
    logger = logging.getLogger(__name__)
    provider = _get_current_provider()
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ëª¨ë¸ëª… ê°€ì ¸ì˜¤ê¸°
    if provider == LLMProvider.OPENAI:
        if model is None:
            model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        
        logger.info(f"OpenAI ëª¨ë¸ ì‚¬ìš©: {model}")
        
        return call_openai_api(
            prompt=prompt,
            model=model,
            max_tokens=max_tokens,
            temperature=temperature,
            api_key=api_key
        )
    
    elif provider == LLMProvider.OLLAMA:
        if model is None:
            model_key = os.getenv("OLLAMA_MODEL", "llama3.1:8b")
        else:
            model_key = model
        
        # ì‹¤ì œ ëª¨ë¸ëª… ê°€ì ¸ì˜¤ê¸°
        actual_model = OLLAMA_MODELS.get(model_key, model_key)
        ollama_url = os.getenv("OLLAMA_URL", "http://115.145.178.160:11434/api/chat")
        logger.info(f"Ollama ëª¨ë¸ ì‚¬ìš©: {model_key} -> {actual_model}")
        
        return call_ollama_api(
            prompt=prompt,
            model=actual_model,
            ollama_url=ollama_url,
            temperature=temperature,
            timeout=300
        )
    
    else:
        logger.error(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” LLM ì œê³µì: {provider}")
        return None


# ì‚¬ìš© ì˜ˆì‹œ ë° ë„ì›€ë§
if __name__ == "__main__":
    # ì˜ˆì‹œ ì‚¬ìš©ë²•
    print("=" * 60)
    print("ğŸ¤– LLM API ëª¨ë“ˆ ì •ë³´")
    print("=" * 60)
    
    print("ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µì:", get_available_providers())
    print("ğŸ“Š í˜„ì¬ ëª¨ë¸ ì •ë³´:", get_model_info())
    
    print("\nğŸ“‹ ì§€ì›ë˜ëŠ” Ollama ëª¨ë¸:")
    for i, model in enumerate(get_available_ollama_models(), 1):
        actual = OLLAMA_MODELS[model]
        print(f"   {i:2d}. {model} -> {actual}")
    
    print("\nğŸ“‹ ì§€ì›ë˜ëŠ” OpenAI ëª¨ë¸:")
    for i, model in enumerate(get_available_openai_models(), 1):
        print(f"   {i:2d}. {model}")
    
    print("\nğŸ”§ í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì˜ˆì‹œ:")
    print("   # Ollama ì‚¬ìš©")
    print("   export LLM_PROVIDER=ollama")
    print("   export OLLAMA_MODEL=llama3.1:8b")
    print("   export OLLAMA_URL=http://115.145.178.160:11434/api/chat")
    print()
    print("   # OpenAI ì‚¬ìš©")  
    print("   export LLM_PROVIDER=openai")
    print("   export OPENAI_MODEL=gpt-4o-mini")
    print("   export OPENAI_API_KEY=your_api_key")
    
    print("\nğŸ“ main.py ì‚¬ìš© ì˜ˆì‹œ:")
    print("   # Ollamaë¡œ ì‹¤í–‰")
    print("   LLM_PROVIDER=ollama OLLAMA_MODEL=llama3.1:8b python main.py --input file.yml --output . --mode baseline")
    print()
    print("   # ë‹¤ë¥¸ ëª¨ë¸ë¡œ ì‹¤í–‰")
    print("   LLM_PROVIDER=ollama OLLAMA_MODEL=codegemma:7b python main.py --input file.yml --output . --mode baseline")
    print("   LLM_PROVIDER=ollama OLLAMA_MODEL=codellama:7b python main.py --input file.yml --output . --mode baseline")
    print()
    print("   # OpenAIë¡œ ì‹¤í–‰")  
    print("   LLM_PROVIDER=openai OPENAI_MODEL=gpt-4o python main.py --input file.yml --output . --mode baseline")
    
    print("=" * 60)
