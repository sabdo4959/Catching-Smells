#!/usr/bin/env python3
"""
ë² ì´ìŠ¤ë¼ì¸ í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ê¸°ì¡´ì— ìƒì„±ëœ í…ŒìŠ¤íŠ¸ íŒŒì¼ë“¤ì„ ì‚¬ìš©í•˜ì—¬ í‰ê°€ ì‹œìŠ¤í…œì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import logging
import sys
import os
from pathlib import Path

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from evaluation.evaluator import BaselineEvaluator


def test_single_evaluation():
    """ë‹¨ì¼ íŒŒì¼ í‰ê°€ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ë‹¨ì¼ íŒŒì¼ í‰ê°€ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
    test_dir = Path(__file__).parent.parent
    original_file = test_dir / "test_smell_detection.yml"
    repaired_file = test_dir / "test_baseline_output.yml"
    
    if not original_file.exists():
        print(f"âŒ ì›ë³¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {original_file}")
        return False
    
    if not repaired_file.exists():
        print(f"âŒ ìˆ˜ì •ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {repaired_file}")
        return False
    
    # í‰ê°€ ì‹¤í–‰
    evaluator = BaselineEvaluator("./test_evaluation_results")
    
    try:
        # ë‹¨ì¼ íŒŒì¼ í‰ê°€
        result = evaluator.evaluate_file(str(original_file), str(repaired_file))
        
        print(f"âœ… ë‹¨ì¼ íŒŒì¼ í‰ê°€ ì™„ë£Œ:")
        print(f"   ì›ë³¸: {result.original_file}")
        print(f"   ìˆ˜ì •ë³¸: {result.repaired_file}")
        print(f"   êµ¬ë¬¸ ì„±ê³µ: {result.syntax_success}")
        print(f"   ìŠ¤ë©œ ë³€í™”: {result.initial_smells_count} -> {result.final_smells_count}")
        print(f"   ìŠ¤ë©œ ì œê±°ìœ¨: {result.smell_removal_rate:.1f}%")
        print(f"   Edit Distance: {result.edit_distance}")
        print(f"   ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.3f}ì´ˆ")
        
        if result.error_message:
            print(f"   ì˜¤ë¥˜: {result.error_message}")
        
        return True
        
    except Exception as e:
        print(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
        return False


def test_group_evaluation():
    """ê·¸ë£¹ í‰ê°€ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª ê·¸ë£¹ í‰ê°€ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ë“¤ ì°¾ê¸°
    test_dir = Path(__file__).parent.parent
    test_files = [
        ("test_smell_detection.yml", "test_baseline_output.yml"),
        ("test_smell_detection2.yml", "test_output2.yml"),
        ("test_smell_detection_final.yml", "test_output3.yml")
    ]
    
    file_pairs = []
    for original, repaired in test_files:
        original_path = test_dir / original
        repaired_path = test_dir / repaired
        
        if original_path.exists() and repaired_path.exists():
            file_pairs.append((str(original_path), str(repaired_path)))
            print(f"âœ… íŒŒì¼ ìŒ ì¶”ê°€: {original} -> {repaired}")
        else:
            print(f"âš ï¸ íŒŒì¼ ìŒ ëˆ„ë½: {original} -> {repaired}")
    
    if not file_pairs:
        print("âŒ í…ŒìŠ¤íŠ¸í•  íŒŒì¼ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    # ê·¸ë£¹ í‰ê°€ ì‹¤í–‰
    evaluator = BaselineEvaluator("./test_evaluation_results")
    
    try:
        summary = evaluator.evaluate_group(file_pairs, "test_baseline")
        
        # ê²°ê³¼ ì¶œë ¥
        evaluator.print_summary(summary)
        
        # ê²°ê³¼ ì €ì¥
        json_file, csv_file = evaluator.save_results(summary)
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥:")
        print(f"   JSON: {json_file}")
        print(f"   CSV: {csv_file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ê·¸ë£¹ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
        return False


def test_metrics_calculation():
    """í‰ê°€ ì§€í‘œ ê³„ì‚° ë¡œì§ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª í‰ê°€ ì§€í‘œ ê³„ì‚° í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    evaluator = BaselineEvaluator("./test_evaluation_results")
    
    # 1. Target Smell Counting í…ŒìŠ¤íŠ¸
    test_smells = [
        {"message": "- 10. Avoid jobs without timeouts (line: 6)"},
        {"message": "- 3. Use fixed version for runs-on argument (line 7)"},
        {"message": "- 15. Some other smell"},
        {"message": "- 8. Use commit hash instead of tags"}
    ]
    
    target_count = evaluator._count_target_smells(test_smells)
    print(f"Target Smell Count í…ŒìŠ¤íŠ¸:")
    print(f"   ì…ë ¥: {len(test_smells)}ê°œ ìŠ¤ë©œ")
    print(f"   íƒ€ê²Ÿ ìŠ¤ë©œ: {target_count}ê°œ (ì˜ˆìƒ: 2ê°œ - #10, #15)")
    
    # 2. Edit Distance ê³„ì‚° í…ŒìŠ¤íŠ¸
    test_dir = Path(__file__).parent.parent
    original_file = test_dir / "test_smell_detection.yml"
    repaired_file = test_dir / "test_baseline_output.yml"
    
    if original_file.exists() and repaired_file.exists():
        edit_distance = evaluator._calculate_edit_distance(str(original_file), str(repaired_file))
        print(f"Edit Distance í…ŒìŠ¤íŠ¸:")
        print(f"   ì›ë³¸: {original_file.name}")
        print(f"   ìˆ˜ì •ë³¸: {repaired_file.name}")
        print(f"   Edit Distance: {edit_distance}")
    else:
        print("âš ï¸ Edit Distance í…ŒìŠ¤íŠ¸ìš© íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    return True


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("ğŸš€ ë² ì´ìŠ¤ë¼ì¸ í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    success_count = 0
    total_tests = 3
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    if test_single_evaluation():
        success_count += 1
    
    if test_group_evaluation():
        success_count += 1
    
    if test_metrics_calculation():
        success_count += 1
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ¯ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {success_count}/{total_tests} ì„±ê³µ")
    if success_count == total_tests:
        print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
    else:
        print("âŒ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
    
    return success_count == total_tests


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
