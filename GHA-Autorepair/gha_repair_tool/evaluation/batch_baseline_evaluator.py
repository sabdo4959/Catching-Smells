#!/usr/bin/env python3
"""
ë² ì´ìŠ¤ë¼ì¸ ë°°ì¹˜ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ì´ë¯¸ ë³µêµ¬ëœ ë² ì´ìŠ¤ë¼ì¸ íŒŒì¼ë“¤ì„ í‰ê°€í•©ë‹ˆë‹¤.
"""

import logging
import argparse
import sys
import os
from pathlib import Path
from typing import List, Tuple, Dict
import time
import json
from datetime import datetime

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from evaluation.evaluator import BaselineEvaluator


class BaselineBatchEvaluator:
    """ë² ì´ìŠ¤ë¼ì¸ ë°°ì¹˜ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, output_dir: str = "./evaluation_results/baseline"):
        """
        Args:
            output_dir: ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
        """
        self.logger = logging.getLogger(__name__)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.evaluator = BaselineEvaluator(str(self.output_dir))
    
    def evaluate_baseline_files(self, original_dir: str, repaired_dir: str, 
                               max_files: int = None) -> Dict[str, any]:
        """
        ë² ì´ìŠ¤ë¼ì¸ ë³µêµ¬ëœ íŒŒì¼ë“¤ì„ í‰ê°€í•©ë‹ˆë‹¤.
        
        Args:
            original_dir: ì›ë³¸ íŒŒì¼ ë””ë ‰í† ë¦¬
            repaired_dir: ë² ì´ìŠ¤ë¼ì¸ ë³µêµ¬ëœ íŒŒì¼ ë””ë ‰í† ë¦¬
            max_files: í‰ê°€í•  ìµœëŒ€ íŒŒì¼ ìˆ˜
            
        Returns:
            Dict: í‰ê°€ ê²°ê³¼ ìš”ì•½
        """
        original_path = Path(original_dir)
        repaired_path = Path(repaired_dir)
        
        if not original_path.exists():
            raise FileNotFoundError(f"ì›ë³¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {original_dir}")
        if not repaired_path.exists():
            raise FileNotFoundError(f"ë³µêµ¬ëœ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {repaired_dir}")
        
        # ì›ë³¸ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        original_files = list(original_path.glob("*"))
        original_files = [f for f in original_files if f.is_file()]
        
        if max_files:
            original_files = original_files[:max_files]
        
        self.logger.info(f"í‰ê°€ ì‹œì‘: {len(original_files)}ê°œ ì›ë³¸ íŒŒì¼")
        
        # íŒŒì¼ ìŒ ë§¤ì¹­
        file_pairs = []
        missing_files = []
        
        for original_file in original_files:
            # ë² ì´ìŠ¤ë¼ì¸ ë³µêµ¬ íŒŒì¼ëª… íŒ¨í„´: {íŒŒì¼ëª…}_baseline_repaired.yml
            repaired_file = repaired_path / f"{original_file.name}_baseline_repaired.yml"
            
            if repaired_file.exists():
                file_pairs.append((str(original_file), str(repaired_file)))
                self.logger.debug(f"íŒŒì¼ ìŒ ë§¤ì¹­: {original_file.name} -> {repaired_file.name}")
            else:
                missing_files.append(str(original_file))
                self.logger.warning(f"ë³µêµ¬ íŒŒì¼ ì—†ìŒ: {repaired_file}")
        
        if not file_pairs:
            raise ValueError(f"ë§¤ì¹­ë˜ëŠ” íŒŒì¼ ìŒì´ ì—†ìŠµë‹ˆë‹¤: {original_dir} <-> {repaired_dir}")
        
        self.logger.info(f"ë§¤ì¹­ëœ íŒŒì¼ ìŒ: {len(file_pairs)}ê°œ")
        if missing_files:
            self.logger.warning(f"ëˆ„ë½ëœ íŒŒì¼: {len(missing_files)}ê°œ")
        
        # í‰ê°€ ì‹¤í–‰
        start_time = datetime.now()
        evaluation_summary = self.evaluator.evaluate_group(
            file_pairs, 
            group_name="llama3.1_8b_baseline"
        )
        evaluation_time = (datetime.now() - start_time).total_seconds()
        
        # ê²°ê³¼ ì €ì¥
        json_file, csv_file = self.evaluator.save_results(evaluation_summary)
        
        # ìš”ì•½ ì¶œë ¥
        self.evaluator.print_summary(evaluation_summary)
        
        # ìƒì„¸ ê²°ê³¼ ë°˜í™˜
        result = {
            'evaluation_summary': evaluation_summary.__dict__,
            'evaluation_time': evaluation_time,
            'total_original_files': len(original_files),
            'matched_file_pairs': len(file_pairs),
            'missing_files': missing_files,
            'success_rate': (len(file_pairs) / len(original_files)) * 100.0 if original_files else 0.0,
            'json_report': str(json_file),
            'csv_report': str(csv_file),
            'timestamp': datetime.now().isoformat()
        }
        
        self.logger.info(f"í‰ê°€ ì™„ë£Œ: {evaluation_time:.1f}ì´ˆ")
        self.logger.info(f"íŒŒì¼ ë§¤ì¹­ë¥ : {result['success_rate']:.1f}% ({len(file_pairs)}/{len(original_files)})")
        
        return result


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ë² ì´ìŠ¤ë¼ì¸ ë°°ì¹˜ í‰ê°€ ë„êµ¬")
    
    parser.add_argument("--original-dir", required=True, help="ì›ë³¸ íŒŒì¼ ë””ë ‰í† ë¦¬")
    parser.add_argument("--repaired-dir", required=True, help="ë² ì´ìŠ¤ë¼ì¸ ë³µêµ¬ëœ íŒŒì¼ ë””ë ‰í† ë¦¬")
    parser.add_argument("--max-files", type=int, help="í‰ê°€í•  ìµœëŒ€ íŒŒì¼ ìˆ˜")
    parser.add_argument("--output-dir", default="./evaluation_results/llama3.1_8b_baseline", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--log-level", default="INFO", choices=["DEBUG", "INFO", "WARNING", "ERROR"])
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=getattr(logging, args.log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        evaluator = BaselineBatchEvaluator(args.output_dir)
        
        result = evaluator.evaluate_baseline_files(
            original_dir=args.original_dir,
            repaired_dir=args.repaired_dir,
            max_files=args.max_files
        )
        
        eval_summary = result['evaluation_summary']
        
        print(f"\nğŸ‰ ë² ì´ìŠ¤ë¼ì¸ ë°°ì¹˜ í‰ê°€ ì™„ë£Œ!")
        print(f"ì›ë³¸ íŒŒì¼: {result['total_original_files']}")
        print(f"ë§¤ì¹­ëœ íŒŒì¼ ìŒ: {result['matched_file_pairs']}")
        print(f"íŒŒì¼ ë§¤ì¹­ë¥ : {result['success_rate']:.1f}%")
        print(f"í‰ê°€ ì‹œê°„: {result['evaluation_time']:.1f}ì´ˆ")
        
        print(f"\nğŸ“Š í‰ê°€ ê²°ê³¼:")
        print(f"êµ¬ë¬¸ ì„±ê³µë¥ : {eval_summary.get('syntax_success_rate', 0):.1f}%")
        print(f"í‰ê·  ìŠ¤ë©œ ì œê±°ìœ¨: {eval_summary.get('avg_smell_removal_rate', 0):.1f}%")
        print(f"í‰ê·  Edit Distance: {eval_summary.get('avg_edit_distance', 0):.1f}")
        
        print(f"\nğŸ“ ê²°ê³¼ íŒŒì¼:")
        print(f"JSON ë³´ê³ ì„œ: {result['json_report']}")
        print(f"CSV ë³´ê³ ì„œ: {result['csv_report']}")
        
        if result['missing_files']:
            print(f"\nâš ï¸  ëˆ„ë½ëœ íŒŒì¼ {len(result['missing_files'])}ê°œ:")
            for missing in result['missing_files'][:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                print(f"  - {Path(missing).name}")
            if len(result['missing_files']) > 5:
                print(f"  ... ë° {len(result['missing_files']) - 5}ê°œ ë”")
        
    except Exception as e:
        logging.error(f"ë°°ì¹˜ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
