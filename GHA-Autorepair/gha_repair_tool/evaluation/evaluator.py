#!/usr/bin/env python3
"""
ë² ì´ìŠ¤ë¼ì¸ í‰ê°€ ì‹œìŠ¤í…œ

3ê°€ì§€ í‰ê°€ ì§€í‘œë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤:
1. êµ¬ë¬¸ ì„±ê³µë¥  (Syntax Success Rate %)
2. íƒ€ê²Ÿ ìŠ¤ë©œ ì œê±°ìœ¨ (Target Smell Removal Rate %)
3. ìˆ˜ì • ë²”ìœ„ ì ì ˆì„± (Edit Scope Appropriateness)
"""

import logging
import json
import csv
import yaml
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime
import difflib

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils import process_runner, yaml_parser


@dataclass
class EvaluationResult:
    """ë‹¨ì¼ íŒŒì¼ì— ëŒ€í•œ í‰ê°€ ê²°ê³¼"""
    file_path: str
    original_file: str
    repaired_file: str
    
    # 1. êµ¬ë¬¸ ì„±ê³µë¥ 
    syntax_success: bool
    actionlint_errors: List[Dict]
    
    # 2. íƒ€ê²Ÿ ìŠ¤ë©œ ì œê±°ìœ¨
    initial_smells_count: int
    final_smells_count: int
    smell_removal_rate: float
    
    # 3. ìˆ˜ì • ë²”ìœ„ ì ì ˆì„±
    edit_distance: int
    
    # ë©”íƒ€ë°ì´í„°
    evaluation_time: str
    processing_time: float
    error_message: Optional[str] = None


@dataclass
class GroupEvaluationSummary:
    """ê·¸ë£¹ ì „ì²´ í‰ê°€ ìš”ì•½"""
    group_name: str
    total_files: int
    
    # 1. êµ¬ë¬¸ ì„±ê³µë¥ 
    syntax_success_count: int
    syntax_success_rate: float
    
    # 2. íƒ€ê²Ÿ ìŠ¤ë©œ ì œê±°ìœ¨
    avg_smell_removal_rate: float
    
    # 3. ìˆ˜ì • ë²”ìœ„ ì ì ˆì„±
    avg_edit_distance: float
    
    # ìƒì„¸ í†µê³„
    detailed_results: List[EvaluationResult]
    evaluation_time: str


class BaselineEvaluator:
    """ë² ì´ìŠ¤ë¼ì¸ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, output_dir: str = "./evaluation_results"):
        """
        Args:
            output_dir: í‰ê°€ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
        """
        self.logger = logging.getLogger(__name__)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # íƒ€ê²Ÿ ìŠ¤ë©œ ë²ˆí˜¸ (baselineê³¼ ë™ì¼)
        self.TARGET_SMELLS = {'1', '4', '5', '10', '11', '15', '16'}
        
    def evaluate_file(self, original_file: str, repaired_file: str) -> EvaluationResult:
        """
        ë‹¨ì¼ íŒŒì¼ ìŒì„ í‰ê°€í•©ë‹ˆë‹¤.
        
        Args:
            original_file: ì›ë³¸ YAML íŒŒì¼ ê²½ë¡œ
            repaired_file: ìˆ˜ì •ëœ YAML íŒŒì¼ ê²½ë¡œ
            
        Returns:
            EvaluationResult: í‰ê°€ ê²°ê³¼
        """
        start_time = datetime.now()
        self.logger.info(f"íŒŒì¼ í‰ê°€ ì‹œì‘: {original_file} -> {repaired_file}")
        
        try:
            # 1. êµ¬ë¬¸ ì„±ê³µë¥  í‰ê°€
            syntax_success, actionlint_errors = self._evaluate_syntax_success(repaired_file)
            
            # 2. íƒ€ê²Ÿ ìŠ¤ë©œ ì œê±°ìœ¨ í‰ê°€
            initial_smells, final_smells, removal_rate = self._evaluate_smell_removal(
                original_file, repaired_file, syntax_success
            )
            
            # 3. ìˆ˜ì • ë²”ìœ„ ì ì ˆì„± í‰ê°€
            edit_distance = self._calculate_edit_distance(original_file, repaired_file)
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            result = EvaluationResult(
                file_path=original_file,
                original_file=original_file,
                repaired_file=repaired_file,
                syntax_success=syntax_success,
                actionlint_errors=actionlint_errors,
                initial_smells_count=initial_smells,
                final_smells_count=final_smells,
                smell_removal_rate=removal_rate,
                edit_distance=edit_distance,
                evaluation_time=start_time.isoformat(),
                processing_time=processing_time
            )
            
            self.logger.info(f"íŒŒì¼ í‰ê°€ ì™„ë£Œ: {processing_time:.2f}ì´ˆ")
            return result
            
        except Exception as e:
            self.logger.error(f"íŒŒì¼ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return EvaluationResult(
                file_path=original_file,
                original_file=original_file,
                repaired_file=repaired_file,
                syntax_success=False,
                actionlint_errors=[],
                initial_smells_count=0,
                final_smells_count=0,
                smell_removal_rate=0.0,
                edit_distance=0,
                evaluation_time=start_time.isoformat(),
                processing_time=processing_time,
                error_message=str(e)
            )
    
    def _evaluate_syntax_success(self, repaired_file: str) -> Tuple[bool, List[Dict]]:
        """êµ¬ë¬¸ ì„±ê³µë¥  í‰ê°€"""
        self.logger.debug(f"êµ¬ë¬¸ ê²€ì‚¬ ì‹œì‘: {repaired_file}")
        
        # actionlint ì‹¤í–‰
        result = process_runner.run_actionlint(repaired_file)
        
        if result.get("success", True):
            # ì„±ê³µ: ì˜¤ë¥˜ ì—†ìŒ
            self.logger.debug("êµ¬ë¬¸ ê²€ì‚¬ í†µê³¼")
            return True, []
        else:
            # ì‹¤íŒ¨: ì˜¤ë¥˜ ìˆìŒ
            errors = result.get("errors", [])
            
            # syntax-checkì™€ expression íƒ€ì…ì˜ ì˜¤ë¥˜ë§Œ êµ¬ë¬¸ ì˜¤ë¥˜ë¡œ ê°„ì£¼
            syntax_errors = [
                error for error in errors 
                if isinstance(error, dict) and error.get('kind') in ['syntax-check', 'expression']
            ]
            
            self.logger.debug(f"êµ¬ë¬¸ ì˜¤ë¥˜ {len(syntax_errors)}ê°œ ë°œê²¬ (syntax-check, expressionë§Œ)")
            if syntax_errors:
                self.logger.debug(f"êµ¬ë¬¸ ì˜¤ë¥˜ ì¢…ë¥˜: {[e.get('kind', 'unknown') for e in syntax_errors[:3]]}")
            
            return len(syntax_errors) == 0, syntax_errors
    
    def _evaluate_smell_removal(self, original_file: str, repaired_file: str, 
                               syntax_success: bool) -> Tuple[int, int, float]:
        """íƒ€ê²Ÿ ìŠ¤ë©œ ì œê±°ìœ¨ í‰ê°€"""
        self.logger.debug(f"ìŠ¤ë©œ ì œê±°ìœ¨ í‰ê°€ ì‹œì‘: {original_file} -> {repaired_file}")
        
        if not syntax_success:
            # êµ¬ë¬¸ ì‹¤íŒ¨í•œ íŒŒì¼ì€ 0% ì²˜ë¦¬
            self.logger.debug("êµ¬ë¬¸ ì‹¤íŒ¨ë¡œ ì¸í•œ ìŠ¤ë©œ ì œê±°ìœ¨ 0%")
            return 0, 0, 0.0
        
        try:
            # ì›ë³¸ íŒŒì¼ì˜ íƒ€ê²Ÿ ìŠ¤ë©œ ê°œìˆ˜
            original_result = process_runner.run_smell_detector(original_file)
            original_smells = self._count_target_smells(original_result.get("smells", []))
            
            # ìˆ˜ì •ëœ íŒŒì¼ì˜ íƒ€ê²Ÿ ìŠ¤ë©œ ê°œìˆ˜
            repaired_result = process_runner.run_smell_detector(repaired_file)
            final_smells = self._count_target_smells(repaired_result.get("smells", []))
            
            # ì œê±°ìœ¨ ê³„ì‚°
            if original_smells == 0:
                # ì›ë³¸ì— ìŠ¤ë©œì´ ì—†ì—ˆë˜ ê²½ìš°
                if final_smells == 0:
                    removal_rate = 100.0  # ì™„ë²½ ìƒíƒœ ìœ ì§€
                else:
                    removal_rate = 0.0    # ìƒˆë¡œìš´ ìŠ¤ë©œ ìƒì„± (ì‹¤íŒ¨)
                    self.logger.debug(f"ìŠ¤ë©œ ì¶”ê°€ë¨: 0 -> {final_smells}")
            else:
                # ì›ë³¸ì— ìŠ¤ë©œì´ ìˆì—ˆë˜ ê²½ìš°
                if final_smells <= original_smells:
                    removal_rate = ((original_smells - final_smells) / original_smells) * 100.0
                else:
                    # ìŠ¤ë©œì´ ëŠ˜ì–´ë‚œ ê²½ìš°: 0%ë¡œ ì²˜ë¦¬ (ì‹¤íŒ¨)
                    removal_rate = 0.0
                    self.logger.debug(f"ìŠ¤ë©œ ì¦ê°€: {original_smells} -> {final_smells}")
            
            self.logger.debug(f"ìŠ¤ë©œ ë³€í™”: {original_smells} -> {final_smells} (ì œê±°ìœ¨: {removal_rate:.1f}%)")
            return original_smells, final_smells, removal_rate
            
        except Exception as e:
            self.logger.error(f"ìŠ¤ë©œ ì œê±°ìœ¨ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return 0, 0, 0.0
    
    def _count_target_smells(self, smells: List[Dict]) -> int:
        """íƒ€ê²Ÿ ìŠ¤ë©œë§Œ ì¹´ìš´íŠ¸"""
        target_count = 0
        for smell in smells:
            if isinstance(smell, dict):
                message = smell.get('message', '')
                # ìŠ¤ë©œ ë²ˆí˜¸ ì¶”ì¶œ (ì˜ˆ: "- 10. Avoid jobs without timeouts")
                for target_num in self.TARGET_SMELLS:
                    if f"- {target_num}." in message or f"#{target_num}" in message:
                        target_count += 1
                        break
        return target_count
    
    def _calculate_edit_distance(self, original_file: str, repaired_file: str) -> int:
        """ìˆ˜ì • ë²”ìœ„ ì ì ˆì„± (Edit Distance) ê³„ì‚°"""
        self.logger.debug(f"Edit Distance ê³„ì‚°: {original_file} -> {repaired_file}")
        
        try:
            # íŒŒì¼ ë‚´ìš© ì½ê¸°
            with open(original_file, 'r', encoding='utf-8') as f:
                original_lines = f.readlines()
            
            with open(repaired_file, 'r', encoding='utf-8') as f:
                repaired_lines = f.readlines()
            
            # SequenceMatcherë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê¸°ë°˜ ê±°ë¦¬ ê³„ì‚°
            matcher = difflib.SequenceMatcher(None, original_lines, repaired_lines)
            
            # í¸ì§‘ ê±°ë¦¬ ê·¼ì‚¬ ê³„ì‚° (ì‚½ì… + ì‚­ì œ + êµì²´)
            opcodes = matcher.get_opcodes()
            edit_distance = 0
            
            for op, i1, i2, j1, j2 in opcodes:
                if op == 'replace':
                    edit_distance += max(i2 - i1, j2 - j1)
                elif op == 'delete':
                    edit_distance += i2 - i1
                elif op == 'insert':
                    edit_distance += j2 - j1
                # 'equal'ì¸ ê²½ìš°ëŠ” ê±°ë¦¬ì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ
            
            self.logger.debug(f"Edit Distance: {edit_distance}")
            return edit_distance
            
        except Exception as e:
            self.logger.error(f"Edit Distance ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return 0
    
    def evaluate_group(self, file_pairs: List[Tuple[str, str]], 
                      group_name: str = "baseline") -> GroupEvaluationSummary:
        """
        íŒŒì¼ ê·¸ë£¹ ì „ì²´ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
        
        Args:
            file_pairs: (ì›ë³¸íŒŒì¼, ìˆ˜ì •íŒŒì¼) ìŒì˜ ë¦¬ìŠ¤íŠ¸
            group_name: ê·¸ë£¹ ì´ë¦„
            
        Returns:
            GroupEvaluationSummary: ê·¸ë£¹ í‰ê°€ ìš”ì•½
        """
        self.logger.info(f"ê·¸ë£¹ í‰ê°€ ì‹œì‘: {group_name} ({len(file_pairs)}ê°œ íŒŒì¼)")
        
        detailed_results = []
        syntax_success_count = 0
        total_smell_removal_rate = 0.0
        total_edit_distance = 0.0
        successful_files_for_edit = 0
        
        for original_file, repaired_file in file_pairs:
            result = self.evaluate_file(original_file, repaired_file)
            detailed_results.append(result)
            
            # í†µê³„ ëˆ„ì 
            if result.syntax_success:
                syntax_success_count += 1
                total_edit_distance += result.edit_distance
                successful_files_for_edit += 1
            
            total_smell_removal_rate += result.smell_removal_rate
        
        # í‰ê·  ê³„ì‚°
        total_files = len(file_pairs)
        syntax_success_rate = (syntax_success_count / total_files) * 100.0 if total_files > 0 else 0.0
        avg_smell_removal_rate = total_smell_removal_rate / total_files if total_files > 0 else 0.0
        avg_edit_distance = total_edit_distance / successful_files_for_edit if successful_files_for_edit > 0 else 0.0
        
        summary = GroupEvaluationSummary(
            group_name=group_name,
            total_files=total_files,
            syntax_success_count=syntax_success_count,
            syntax_success_rate=syntax_success_rate,
            avg_smell_removal_rate=avg_smell_removal_rate,
            avg_edit_distance=avg_edit_distance,
            detailed_results=detailed_results,
            evaluation_time=datetime.now().isoformat()
        )
        
        self.logger.info(f"ê·¸ë£¹ í‰ê°€ ì™„ë£Œ: {group_name}")
        self.logger.info(f"  êµ¬ë¬¸ ì„±ê³µë¥ : {syntax_success_rate:.1f}% ({syntax_success_count}/{total_files})")
        self.logger.info(f"  í‰ê·  ìŠ¤ë©œ ì œê±°ìœ¨: {avg_smell_removal_rate:.1f}%")
        self.logger.info(f"  í‰ê·  Edit Distance: {avg_edit_distance:.1f}")
        
        return summary
    
    def save_results(self, summary: GroupEvaluationSummary) -> Tuple[str, str]:
        """
        í‰ê°€ ê²°ê³¼ë¥¼ ë² ì´ìŠ¤ë¼ì¸ í˜•ì‹ê³¼ ì¼ì¹˜í•˜ê²Œ JSON íŒŒì¼ 2ê°œë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            summary: ê·¸ë£¹ í‰ê°€ ìš”ì•½
            
        Returns:
            Tuple[str, str]: (ìš”ì•½ JSON íŒŒì¼ ê²½ë¡œ, ìƒì„¸ JSON íŒŒì¼ ê²½ë¡œ)
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # í†µê³„ ê³„ì‚°
        smell_removal_rates = [result.smell_removal_rate for result in summary.detailed_results]
        edit_distances = [result.edit_distance for result in summary.detailed_results 
                         if result.syntax_success]  # êµ¬ë¬¸ ì„±ê³µí•œ ê²½ìš°ë§Œ
        
        import statistics
        
        # 1. ìš”ì•½ JSON íŒŒì¼ ì €ì¥ (ë² ì´ìŠ¤ë¼ì¸ í˜•ì‹ê³¼ ë™ì¼)
        summary_data = {
            "evaluation_date": summary.evaluation_time,
            "total_files": summary.total_files,
            "original_directory": "data_original",
            "repaired_directory": "data_repair_two_phase",
            "syntax_success_rate": round(summary.syntax_success_rate, 2),
            "average_smell_removal_rate": round(summary.avg_smell_removal_rate, 2),
            "average_edit_distance": round(summary.avg_edit_distance, 2),
            "syntax_successes": summary.syntax_success_count,
            "syntax_failures": summary.total_files - summary.syntax_success_count,
            "smell_removal_stats": {
                "min": min(smell_removal_rates) if smell_removal_rates else 0.0,
                "max": max(smell_removal_rates) if smell_removal_rates else 0.0,
                "median": statistics.median(smell_removal_rates) if smell_removal_rates else 0.0,
                "stdev": statistics.stdev(smell_removal_rates) if len(smell_removal_rates) > 1 else 0.0
            },
            "edit_distance_stats": {
                "min": min(edit_distances) if edit_distances else 0,
                "max": max(edit_distances) if edit_distances else 0,
                "median": statistics.median(edit_distances) if edit_distances else 0,
                "stdev": statistics.stdev(edit_distances) if len(edit_distances) > 1 else 0.0
            }
        }
        
        summary_file = self.output_dir / f"{summary.group_name}_summary_{timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
        
        # 2. ìƒì„¸ JSON íŒŒì¼ ì €ì¥ (ë² ì´ìŠ¤ë¼ì¸ í˜•ì‹ê³¼ ë™ì¼ - ë°°ì—´ í˜•íƒœ)
        detailed_data = [asdict(result) for result in summary.detailed_results]
        
        detailed_file = self.output_dir / f"{summary.group_name}_detailed_{timestamp}.json"
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_data, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        self.logger.info(f"  ìš”ì•½ JSON: {summary_file}")
        self.logger.info(f"  ìƒì„¸ JSON: {detailed_file}")
        
        return str(summary_file), str(detailed_file)
    
    def print_summary(self, summary: GroupEvaluationSummary):
        """í‰ê°€ ìš”ì•½ì„ ì½˜ì†”ì— ì¶œë ¥í•©ë‹ˆë‹¤."""
        print(f"\nğŸ“Š {summary.group_name} ê·¸ë£¹ í‰ê°€ ê²°ê³¼")
        print("=" * 50)
        print(f"ì´ íŒŒì¼ ìˆ˜: {summary.total_files}")
        print()
        print(f"1. êµ¬ë¬¸ ì„±ê³µë¥ : {summary.syntax_success_rate:.1f}% ({summary.syntax_success_count}/{summary.total_files})")
        print(f"2. í‰ê·  íƒ€ê²Ÿ ìŠ¤ë©œ ì œê±°ìœ¨: {summary.avg_smell_removal_rate:.1f}%")
        print(f"3. í‰ê·  ìˆ˜ì • ë²”ìœ„ (Edit Distance): {summary.avg_edit_distance:.1f}")
        print(f"\ní‰ê°€ ì™„ë£Œ ì‹œê°: {summary.evaluation_time}")


def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ë² ì´ìŠ¤ë¼ì¸ í‰ê°€ ë„êµ¬")
    parser.add_argument("--original", required=True, help="ì›ë³¸ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--repaired", required=True, help="ìˆ˜ì •ëœ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output-dir", default="./evaluation_results", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--group-name", default="test", help="ê·¸ë£¹ ì´ë¦„")
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    # í‰ê°€ ì‹¤í–‰
    evaluator = BaselineEvaluator(args.output_dir)
    
    file_pairs = [(args.original, args.repaired)]
    summary = evaluator.evaluate_group(file_pairs, args.group_name)
    
    # ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
    evaluator.print_summary(summary)
    evaluator.save_results(summary)


if __name__ == "__main__":
    main()
