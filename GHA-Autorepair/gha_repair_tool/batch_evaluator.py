#!/usr/bin/env python3
"""
ë°°ì¹˜ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

data_originalê³¼ data_repair_baseline ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ì„ ë¹„êµí•˜ì—¬
3ê°€ì§€ í‰ê°€ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List
import statistics

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from evaluation.evaluator import BaselineEvaluator


def find_matching_files(original_dir: Path, repaired_dir: Path) -> List[tuple]:
    """
    ì›ë³¸ê³¼ ë³µêµ¬ëœ íŒŒì¼ë“¤ì˜ ë§¤ì¹­ ìŒì„ ì°¾ìŠµë‹ˆë‹¤.
    
    Returns:
        List[tuple]: (ì›ë³¸íŒŒì¼ê²½ë¡œ, ë³µêµ¬íŒŒì¼ê²½ë¡œ) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
    """
    matches = []
    
    for original_file in original_dir.glob("*"):
        if not original_file.is_file():
            continue
            
        # ë³µêµ¬ëœ íŒŒì¼ëª… íŒ¨í„´: {ì›ë³¸ëª…}_baseline_repaired.yml
        repaired_name = f"{original_file.name}_baseline_repaired.yml"
        repaired_file = repaired_dir / repaired_name
        
        if repaired_file.exists():
            matches.append((str(original_file), str(repaired_file)))
    
    return matches


def batch_evaluate(original_dir: str, repaired_dir: str, output_dir: str) -> Dict:
    """
    ë°°ì¹˜ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    logger = logging.getLogger(__name__)
    
    # ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
    original_path = Path(original_dir)
    repaired_path = Path(repaired_dir)
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # ë§¤ì¹­ íŒŒì¼ ì°¾ê¸°
    file_pairs = find_matching_files(original_path, repaired_path)
    logger.info(f"ë§¤ì¹­ëœ íŒŒì¼ ìŒ: {len(file_pairs)}ê°œ")
    
    if not file_pairs:
        logger.error("ë§¤ì¹­ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    # í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    evaluator = BaselineEvaluator()
    
    # ê²°ê³¼ ì €ì¥ìš©
    results = []
    syntax_successes = []
    smell_removal_rates = []
    edit_distances = []
    
    # ê° íŒŒì¼ ìŒ í‰ê°€
    for i, (original_file, repaired_file) in enumerate(file_pairs, 1):
        logger.info(f"[{i}/{len(file_pairs)}] í‰ê°€ ì¤‘: {Path(original_file).name}")
        
        try:
            # í‰ê°€ ì‹¤í–‰
            result = evaluator.evaluate_file(original_file, repaired_file)
            results.append(result)
            
            # ì§€í‘œ ìˆ˜ì§‘
            syntax_successes.append(1 if result.syntax_success else 0)
            smell_removal_rates.append(result.smell_removal_rate)
            edit_distances.append(result.edit_distance)
            
            logger.info(f"  êµ¬ë¬¸ ì„±ê³µ: {result.syntax_success}")
            logger.info(f"  ìŠ¤ë©œ ì œê±°ìœ¨: {result.smell_removal_rate:.1f}%")
            logger.info(f"  í¸ì§‘ ê±°ë¦¬: {result.edit_distance}")
            
        except Exception as e:
            logger.error(f"í‰ê°€ ì‹¤íŒ¨ ({Path(original_file).name}): {e}")
    
    # ì „ì²´ í†µê³„ ê³„ì‚°
    total_files = len(results)
    syntax_success_rate = (sum(syntax_successes) / total_files * 100) if total_files > 0 else 0
    avg_smell_removal_rate = statistics.mean(smell_removal_rates) if smell_removal_rates else 0
    avg_edit_distance = statistics.mean(edit_distances) if edit_distances else 0
    
    # ê²°ê³¼ ìš”ì•½
    summary = {
        "evaluation_date": datetime.now().isoformat(),
        "total_files": total_files,
        "original_directory": str(original_path),
        "repaired_directory": str(repaired_path),
        
        # ì£¼ìš” ì§€í‘œ
        "syntax_success_rate": round(syntax_success_rate, 2),
        "average_smell_removal_rate": round(avg_smell_removal_rate, 2),
        "average_edit_distance": round(avg_edit_distance, 2),
        
        # ìƒì„¸ í†µê³„
        "syntax_successes": sum(syntax_successes),
        "syntax_failures": total_files - sum(syntax_successes),
        "smell_removal_stats": {
            "min": min(smell_removal_rates) if smell_removal_rates else 0,
            "max": max(smell_removal_rates) if smell_removal_rates else 0,
            "median": statistics.median(smell_removal_rates) if smell_removal_rates else 0,
            "stdev": statistics.stdev(smell_removal_rates) if len(smell_removal_rates) > 1 else 0
        },
        "edit_distance_stats": {
            "min": min(edit_distances) if edit_distances else 0,
            "max": max(edit_distances) if edit_distances else 0,
            "median": statistics.median(edit_distances) if edit_distances else 0,
            "stdev": statistics.stdev(edit_distances) if len(edit_distances) > 1 else 0
        }
    }
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. ìš”ì•½ ê²°ê³¼ JSON
    summary_file = output_path / f"evaluation_summary_{timestamp}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    # 2. ìƒì„¸ ê²°ê³¼ JSON
    detailed_file = output_path / f"evaluation_detailed_{timestamp}.json"
    detailed_results = [result.__dict__ for result in results]
    with open(detailed_file, 'w', encoding='utf-8') as f:
        json.dump(detailed_results, f, indent=2, ensure_ascii=False)
    
    # ê²°ê³¼ ì¶œë ¥
    logger.info("=" * 60)
    logger.info("ë°°ì¹˜ í‰ê°€ ì™„ë£Œ!")
    logger.info(f"ì´ íŒŒì¼: {total_files}")
    logger.info(f"êµ¬ë¬¸ ì„±ê³µë¥ : {syntax_success_rate:.2f}% ({sum(syntax_successes)}/{total_files})")
    logger.info(f"í‰ê·  ìŠ¤ë©€ ì œê±°ìœ¨: {avg_smell_removal_rate:.2f}%")
    logger.info(f"í‰ê·  í¸ì§‘ ê±°ë¦¬: {avg_edit_distance:.2f}")
    logger.info(f"ìš”ì•½ ê²°ê³¼: {summary_file}")
    logger.info(f"ìƒì„¸ ê²°ê³¼: {detailed_file}")
    logger.info("=" * 60)
    
    return summary


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ë°°ì¹˜ í‰ê°€ ë„êµ¬")
    parser.add_argument("--original-dir", required=True, help="ì›ë³¸ ë””ë ‰í† ë¦¬")
    parser.add_argument("--repaired-dir", required=True, help="ë³µêµ¬ëœ ë””ë ‰í† ë¦¬")
    parser.add_argument("--output-dir", default="./evaluation_results", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--log-level", default="INFO", choices=["DEBUG", "INFO", "WARNING", "ERROR"])
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=getattr(logging, args.log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler()]
    )
    
    try:
        summary = batch_evaluate(args.original_dir, args.repaired_dir, args.output_dir)
        
        print(f"\nğŸ‰ ë°°ì¹˜ í‰ê°€ ì™„ë£Œ!")
        print(f"ì´ íŒŒì¼: {summary.get('total_files', 0)}")
        print(f"êµ¬ë¬¸ ì„±ê³µë¥ : {summary.get('syntax_success_rate', 0):.2f}%")
        print(f"í‰ê·  ìŠ¤ë©¸ ì œê±°ìœ¨: {summary.get('average_smell_removal_rate', 0):.2f}%")
        print(f"í‰ê·  í¸ì§‘ ê±°ë¦¬: {summary.get('average_edit_distance', 0):.2f}")
        
        return True
        
    except Exception as e:
        logging.error(f"ë°°ì¹˜ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
